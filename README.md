
## 5. Технические особенности

### 5.1. Сбор и работа с данными
*   **Источник:** Основным источником данных служит файл `data/historical_events.csv`. Он был скомпилирован вручную и с использованием открытых источников (например, Википедия) с акцентом на первую четверть XIX века.
*   **Формат:** CSV файл содержит колонки: `date` (дата события в формате ГГГГ-ММ-ДД или ГГГГ), `event_description` (текстовое описание), `location` (место), `category` (категория).
*   **Офлайн-обработка (Создание Индекса):** Для подготовки данных к RAG используется отдельный скрипт (например, `create_vector_db.ipynb`):
    1.  Скрипт читает `historical_events.csv` с помощью Pandas.
    2.  Загружается модель эмбеддингов (`sentence-transformers/paraphrase-multilingual-mpnet-base-v2`).
    3.  Для каждого события из `event_description` (и опционально других полей) генерируется векторный эмбеддинг.
    4.  Создается индекс FAISS на основе этих эмбеддингов и связанных метаданных (дата, место и т.д.).
    5.  Готовый индекс сохраняется на диск в виде двух файлов (`index.faiss`, `index.pkl`) в папку `faiss_index_historical`.

### 5.2. RAG (Retrieval-Augmented Generation)
*   **Цель:** Найти релевантные исторические события, близкие к выбранной пользователем дате, чтобы передать их LLM в качестве контекста.
*   **Реализация (во время работы приложения):**
    1.  **Загрузка Индекса:** При первом обращении (благодаря `@st.cache_resource`) функция `get_vector_store` из `rag.py` загружает пред-созданный индекс FAISS из файлов `faiss_index_historical/index.faiss` и `faiss_index_historical/index.pkl`. Для загрузки также требуется инициализировать модель эмбеддингов (`get_embeddings_loader`), но она используется только для интерпретации структуры индекса, а не для генерации новых эмбеддингов.
    2.  **Создание Ретривера:** Функция `get_retriever` создает объект-ретривер LangChain на основе загруженного индекса FAISS.
    3.  **Семантический Поиск:** Ретривер используется для поиска событий (`retriever.invoke(query)`). В качестве запроса (`query`) используется текстовое представление даты, например, "События около 7 Сентября 1812". Ретривер находит `k` документов, чьи эмбеддинги наиболее близки к эмбеддингу запроса (семантически похожи).
    4.  **Фильтрация по Дате:** Полученный от ретривера список документов дополнительно фильтруется в функции `generate_news`. Даты из метаданных документов (`doc.metadata['date']`) сравниваются с выбранной пользователем датой, и остаются только те документы, которые попадают в заданное окно (например, +/- 7 дней).
    5.  **Формирование Контекста:** Текстовые описания (`doc.page_content`) отфильтрованных по дате документов объединяются и передаются в промпт LLM.

### 5.3. Модели
*   **Языковая модель (LLM):** Используется `gpt-3.5-turbo` (через совместимый API), отвечающая за генерацию текста новостей, стилизацию и форматирование вывода в JSON.
*   **Модель Эмбеддингов:** `sentence-transformers/paraphrase-multilingual-mpnet-base-v2` (используется только офлайн для создания векторной базы).

### 5.4. Техники промптинга и структурированный вывод
*   **Промптинг:** Используется `ChatPromptTemplate` из LangChain.
    *   **Системный промпт:** Задает роль LLM ("редактор газеты 'Хронографъ'"), основную задачу, требование использовать RAG-контекст, добавлять юмор/стиль, и **строго следовать формату JSON**. Инструкции по формату JSON (`format_instructions`) передаются динамически из Pydantic парсера.
    *   **Пользовательский промпт:** Содержит конкретные параметры запроса (дата, стиль эпохи, желаемое кол-во статей).
*   **Структурированный вывод:**
    *   **Pydantic Модели:** В `models.py` определены модели `NewsArticle` (для одной новости с полями `headline`, `date_location`, `body`, `rubric`, `reporter`) и `NewsReport` (содержащая список `articles`).
    *   **PydanticOutputParser:** Парсер LangChain используется в цепочке после LLM (`prompt | llm | pydantic_parser`). Он берет JSON-ответ от LLM и автоматически валидирует его по схеме `NewsReport`, преобразуя в Python-объект. Это обеспечивает надежность и предсказуемость структуры данных.
    *   **Обработка ошибок парсинга:** В коде предусмотрены попытки повторной генерации (`MAX_RETRIES`) и ручного извлечения JSON из ответа LLM в случае сбоя автоматического парсинга.

### 5.5. Хранилище (Векторная база)
*   **Тип:** FAISS (Facebook AI Similarity Search).
*   **Назначение:** Эффективное хранение векторных эмбеддингов исторических событий и быстрый поиск ближайших соседей (семантически похожих событий).
*   **Использование:** Индекс **создается офлайн** и **загружается при старте** приложения из локальных файлов (`index.faiss`, `index.pkl`). Это решение выбрано для ускорения запуска Streamlit-приложения и снижения потребления ресурсов во время работы, так как генерация эмбеддингов является ресурсоемкой операцией. Загруженный индекс кэшируется в памяти с помощью `@st.cache_resource`.

### 5.6. Логика работы приложения (По шагам)
1.  **Запуск:** Пользователь открывает URL приложения. `app.py` выполняется, рисует UI. Кэшированные ресурсы (`get_vector_store`, `get_embeddings_loader`) пока не загружаются.
2.  **Ввод пользователя:** Пользователь выбирает дату, век, кол-во статей, окно дат.
3.  **Нажатие кнопки:** Пользователь нажимает "Сгенерировать".
4.  **Вызов `generate_news`:** `app.py` вызывает функцию `generate_news` из `generator.py`.
5.  **Загрузка/Получение Ретривера:** `generate_news` вызывает `get_retriever`. Тот вызывает `get_vector_store`.
    *   *При первом вызове:* `get_vector_store` видит, что индекса нет в кэше. Вызывает `get_embeddings_loader` (тот загружает модель эмбеддингов и кэширует ее), затем загружает файлы `index.faiss` и `index.pkl`, создает объект `FAISS` и кэширует его. Streamlit показывает "Running: get_vector_store()...".
    *   *При последующих вызовах:* `get_vector_store` мгновенно возвращает объект `FAISS` из кэша `@st.cache_resource`.
6.  **RAG Поиск:** `generate_news` использует ретривер для семантического поиска событий по текстовому запросу с датой. Получает список `all_docs`.
7.  **Фильтрация по дате:** `generate_news` фильтрует `all_docs`, оставляя только те, чья дата в метаданных попадает в заданное окно (`date_window_days`) от даты пользователя. Получает `filtered_docs`.
8.  **Подготовка контекста:** Выбирается до `num_articles` документов из `filtered_docs`, их тексты объединяются в `context`.
9.  **Создание Цепочки LLM:** `generate_news` вызывает `create_generation_chain`, которая инициализирует LLM, Pydantic парсер и создает цепочку `prompt | llm | pydantic_parser`.
10. **Вызов LLM:** Цепочка выполняется (`chain.invoke`) с подготовленным контекстом, датой, стилем и инструкциями по форматированию.
11. **Парсинг и Валидация:** `PydanticOutputParser` обрабатывает ответ LLM, проверяет соответствие JSON схеме `NewsReport` и возвращает Pydantic объект `result`. Предусмотрены повторные попытки и ручной парсинг при ошибках.
12. **Возврат результата:** `generate_news` возвращает объект `NewsReport` (или пустой) в `app.py`.
13. **Отображение:** `app.py` получает результат и отображает сгенерированные статьи или сообщение об ошибке/отсутствии данных.

## 6. Деплой

### Платформа
Приложение развернуто с использованием **Streamlit Community Cloud**.

### Причина выбора
*   **Бесплатно:** Платформа предоставляет щедрый бесплатный тир для публичных репозиториев GitHub.
*   **Простота:** Идеально подходит для развертывания приложений, написанных на Streamlit. Процесс деплоя сводится к подключению GitHub репозитория и указанию главного файла.
*   **Интеграция с GitHub:** Автоматическое обновление приложения при push'ах в связанную ветку репозитория.
*   **Управление секретами:** Удобный интерфейс для безопасного хранения API ключей (`st.secrets`).

### Взаимодействие с пользователем
Взаимодействие происходит через веб-интерфейс:
*   Пользователь выбирает дату с помощью виджета `st.date_input`.
*   Выбирает стиль эпохи (век) из выпадающего списка `st.selectbox`.
*   Задает желаемое количество новостей и окно дат с помощью слайдеров `st.slider`.
*   Нажимает кнопку `st.button` для запуска генерации.
*   Результат (сгенерированные новости или сообщения об ошибках/статусе) отображается в основной части страницы.

### Ссылки
*   **GitHub репозиторий:** `[ССЫЛКА НА ВАШ РЕПОЗИТОРИЙ]`
*   **Работающее приложение:** `[ССЫЛКА НА РАБОТАЮЩЕЕ ПРИЛОЖЕНИЕ НА STREAMLIT CLOUD]`